// Package main æ¼”ç¤ºæ€§èƒ½ä¼˜åŒ–å’Œå‹æµ‹
// å¹¶å‘å¤„ç†ã€æ‰¹é‡æ“ä½œã€æ€§èƒ½ç›‘æ§
package main

import (
	"context"
	"fmt"
	"log"
	"os"
	"os/signal"
	"sync"
	"sync/atomic"
	"syscall"
	"time"

	"github.com/FangcunMount/qs-server/pkg/messaging"
	_ "github.com/FangcunMount/qs-server/pkg/messaging/nsq"
)

func main() {
	log.Println("=== æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º ===")

	// ========== æ¼”ç¤º 1: åŸºå‡†æµ‹è¯• ==========
	demonstrateBenchmark()
	time.Sleep(2 * time.Second)

	// ========== æ¼”ç¤º 2: å¹¶å‘ä¼˜åŒ– ==========
	demonstrateConcurrency()
	time.Sleep(2 * time.Second)

	// ========== æ¼”ç¤º 3: æ‰¹é‡å¤„ç†ä¼˜åŒ– ==========
	demonstrateBatchProcessing()
	time.Sleep(2 * time.Second)

	// ========== æ¼”ç¤º 4: å†…å­˜ä¼˜åŒ– ==========
	demonstrateMemoryOptimization()

	log.Println("\næŒ‰ Ctrl+C é€€å‡º...")
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
	<-sigChan
}

// ========== æ¼”ç¤º 1: åŸºå‡†æµ‹è¯• ==========

// PerformanceMetrics æ€§èƒ½æŒ‡æ ‡
type PerformanceMetrics struct {
	TotalMessages   int64
	SuccessMessages int64
	FailedMessages  int64
	TotalDuration   time.Duration
	StartTime       time.Time
	EndTime         time.Time
}

func (pm *PerformanceMetrics) Calculate() {
	pm.TotalDuration = pm.EndTime.Sub(pm.StartTime)
}

func (pm *PerformanceMetrics) Report() {
	log.Println("\n========== æ€§èƒ½æŠ¥å‘Š ==========")
	log.Printf("æ€»æ¶ˆæ¯æ•°: %d", pm.TotalMessages)
	log.Printf("æˆåŠŸ: %d (%.2f%%)", pm.SuccessMessages,
		float64(pm.SuccessMessages)/float64(pm.TotalMessages)*100)
	log.Printf("å¤±è´¥: %d (%.2f%%)", pm.FailedMessages,
		float64(pm.FailedMessages)/float64(pm.TotalMessages)*100)
	log.Printf("æ€»è€—æ—¶: %v", pm.TotalDuration)
	log.Printf("ååé‡: %.2f msg/s",
		float64(pm.TotalMessages)/pm.TotalDuration.Seconds())
	log.Printf("å¹³å‡å»¶è¿Ÿ: %.2f ms",
		float64(pm.TotalDuration.Milliseconds())/float64(pm.TotalMessages))
	log.Println("=============================")
}

func demonstrateBenchmark() {
	log.Println("ã€æ¼”ç¤º 1ã€‘åŸºå‡†æµ‹è¯• - æµ‹é‡ååé‡å’Œå»¶è¿Ÿ")

	bus, _ := messaging.NewEventBus(messaging.DefaultConfig())
	defer bus.Close()

	logger := log.New(os.Stdout, "[Benchmark] ", log.LstdFlags)
	router := bus.Router()
	router.AddMiddleware(messaging.LoggerMiddleware(logger))

	metrics := &PerformanceMetrics{}

	handler := func(ctx context.Context, msg *messaging.Message) error {
		atomic.AddInt64(&metrics.SuccessMessages, 1)
		// æ¨¡æ‹Ÿå¤„ç†ï¼ˆéå¸¸å¿«ï¼‰
		return msg.Ack()
	}

	router.AddHandler("demo.benchmark", "benchmark-demo", handler)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	go router.Run(ctx)

	time.Sleep(time.Second)

	// åŸºå‡†æµ‹è¯•å‚æ•°
	messageCount := int64(1000)
	log.Printf("å‘é€ %d æ¡æ¶ˆæ¯è¿›è¡ŒåŸºå‡†æµ‹è¯•...\n", messageCount)

	metrics.StartTime = time.Now()
	metrics.TotalMessages = messageCount

	// å‘é€æ¶ˆæ¯
	for i := int64(0); i < messageCount; i++ {
		msg := fmt.Sprintf("æ¶ˆæ¯-%d", i)
		bus.Publisher().Publish(context.Background(), "demo.benchmark", []byte(msg))
	}

	// ç­‰å¾…å¤„ç†å®Œæˆ
	for {
		processed := atomic.LoadInt64(&metrics.SuccessMessages)
		if processed >= messageCount {
			break
		}
		time.Sleep(100 * time.Millisecond)
	}

	metrics.EndTime = time.Now()
	metrics.Calculate()
	router.Stop()

	metrics.Report()
}

// ========== æ¼”ç¤º 2: å¹¶å‘ä¼˜åŒ– ==========

func demonstrateConcurrency() {
	log.Println("ã€æ¼”ç¤º 2ã€‘å¹¶å‘ä¼˜åŒ– - å¤š Worker å¹¶è¡Œå¤„ç†")

	bus, _ := messaging.NewEventBus(messaging.DefaultConfig())
	defer bus.Close()

	logger := log.New(os.Stdout, "[Concurrency] ", log.LstdFlags)

	// æµ‹è¯•ä¸åŒçš„ Worker æ•°é‡
	workerCounts := []int{1, 2, 4, 8}

	for _, workers := range workerCounts {
		testConcurrency(bus, logger, workers)
		time.Sleep(2 * time.Second)
	}
}

func testConcurrency(bus messaging.EventBus, logger *log.Logger, workers int) {
	log.Printf("æµ‹è¯• %d ä¸ª Worker...\n", workers)

	router := bus.Router()
	router.AddMiddleware(messaging.LoggerMiddleware(logger))

	var processed int64
	handler := func(ctx context.Context, msg *messaging.Message) error {
		// æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
		time.Sleep(10 * time.Millisecond)
		atomic.AddInt64(&processed, 1)
		return msg.Ack()
	}

	// å¯åŠ¨å¤šä¸ª Workerï¼ˆé€šè¿‡å¤šæ¬¡æ³¨å†Œç›¸åŒçš„ Handlerï¼‰
	for i := 0; i < workers; i++ {
		router.AddHandler("demo.concurrency", fmt.Sprintf("worker-%d", i), handler)
	}

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	go router.Run(ctx)

	time.Sleep(time.Second)

	// å‘é€æ¶ˆæ¯
	messageCount := 100
	start := time.Now()

	for i := 0; i < messageCount; i++ {
		msg := fmt.Sprintf("æ¶ˆæ¯-%d", i)
		bus.Publisher().Publish(context.Background(), "demo.concurrency", []byte(msg))
	}

	// ç­‰å¾…å¤„ç†å®Œæˆ
	for atomic.LoadInt64(&processed) < int64(messageCount) {
		time.Sleep(100 * time.Millisecond)
	}

	duration := time.Since(start)
	router.Stop()

	log.Printf("  âœ… %d Workers: å¤„ç† %d æ¡æ¶ˆæ¯ï¼Œè€—æ—¶ %vï¼Œååé‡ %.2f msg/s\n",
		workers, messageCount, duration, float64(messageCount)/duration.Seconds())
}

// ========== æ¼”ç¤º 3: æ‰¹é‡å¤„ç†ä¼˜åŒ– ==========

// BatchProcessor æ‰¹é‡å¤„ç†å™¨
type BatchProcessor struct {
	mu          sync.Mutex
	batch       []*messaging.Message
	batchSize   int
	flushTicker *time.Ticker
	processor   func([]*messaging.Message) error
}

func NewBatchProcessor(batchSize int, flushInterval time.Duration, processor func([]*messaging.Message) error) *BatchProcessor {
	bp := &BatchProcessor{
		batchSize:   batchSize,
		flushTicker: time.NewTicker(flushInterval),
		processor:   processor,
	}

	// å®šæœŸåˆ·æ–°
	go func() {
		for range bp.flushTicker.C {
			bp.Flush()
		}
	}()

	return bp
}

func (bp *BatchProcessor) Add(msg *messaging.Message) error {
	bp.mu.Lock()
	bp.batch = append(bp.batch, msg)
	shouldFlush := len(bp.batch) >= bp.batchSize
	bp.mu.Unlock()

	if shouldFlush {
		return bp.Flush()
	}

	return nil
}

func (bp *BatchProcessor) Flush() error {
	bp.mu.Lock()
	if len(bp.batch) == 0 {
		bp.mu.Unlock()
		return nil
	}

	currentBatch := bp.batch
	bp.batch = nil
	bp.mu.Unlock()

	log.Printf("  ğŸ“¦ æ‰¹é‡å¤„ç† %d æ¡æ¶ˆæ¯", len(currentBatch))
	return bp.processor(currentBatch)
}

func (bp *BatchProcessor) Close() {
	bp.flushTicker.Stop()
	bp.Flush()
}

func demonstrateBatchProcessing() {
	log.Println("ã€æ¼”ç¤º 3ã€‘æ‰¹é‡å¤„ç†ä¼˜åŒ– - æ‰¹é‡æ“ä½œæå‡æ€§èƒ½")

	bus, _ := messaging.NewEventBus(messaging.DefaultConfig())
	defer bus.Close()

	logger := log.New(os.Stdout, "[Batch] ", log.LstdFlags)

	// å¯¹æ¯”å•æ¡å¤„ç† vs æ‰¹é‡å¤„ç†
	log.Println("åœºæ™¯ 1: å•æ¡å¤„ç†")
	testSingleProcessing(bus, logger)

	time.Sleep(2 * time.Second)

	log.Println("\nåœºæ™¯ 2: æ‰¹é‡å¤„ç†ï¼ˆæ¯ 10 æ¡æ‰¹é‡ï¼‰")
	testBatchProcessing(bus, logger)
}

func testSingleProcessing(bus messaging.EventBus, logger *log.Logger) {
	router := bus.Router()
	router.AddMiddleware(messaging.LoggerMiddleware(logger))

	var processed int64
	handler := func(ctx context.Context, msg *messaging.Message) error {
		// æ¨¡æ‹Ÿæ•°æ®åº“å†™å…¥ï¼ˆå•æ¡ï¼‰
		time.Sleep(5 * time.Millisecond)
		atomic.AddInt64(&processed, 1)
		return msg.Ack()
	}

	router.AddHandler("demo.single", "single-demo", handler)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	go router.Run(ctx)

	time.Sleep(time.Second)

	messageCount := 50
	start := time.Now()

	for i := 0; i < messageCount; i++ {
		msg := fmt.Sprintf("æ¶ˆæ¯-%d", i)
		bus.Publisher().Publish(context.Background(), "demo.single", []byte(msg))
	}

	for atomic.LoadInt64(&processed) < int64(messageCount) {
		time.Sleep(100 * time.Millisecond)
	}

	duration := time.Since(start)
	router.Stop()

	log.Printf("  å•æ¡å¤„ç†: %d æ¡æ¶ˆæ¯ï¼Œè€—æ—¶ %v\n", messageCount, duration)
}

func testBatchProcessing(bus messaging.EventBus, logger *log.Logger) {
	router := bus.Router()
	router.AddMiddleware(messaging.LoggerMiddleware(logger))

	var processed int64

	// æ‰¹é‡å¤„ç†å‡½æ•°
	batchProcess := func(messages []*messaging.Message) error {
		// æ¨¡æ‹Ÿæ‰¹é‡æ•°æ®åº“å†™å…¥ï¼ˆæ‰¹é‡æ›´å¿«ï¼‰
		time.Sleep(10 * time.Millisecond)
		atomic.AddInt64(&processed, int64(len(messages)))

		for _, msg := range messages {
			msg.Ack()
		}
		return nil
	}

	batchProcessor := NewBatchProcessor(10, 1*time.Second, batchProcess)
	defer batchProcessor.Close()

	handler := func(ctx context.Context, msg *messaging.Message) error {
		return batchProcessor.Add(msg)
	}

	router.AddHandler("demo.batch", "batch-demo", handler)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()
	go router.Run(ctx)

	time.Sleep(time.Second)

	messageCount := 50
	start := time.Now()

	for i := 0; i < messageCount; i++ {
		msg := fmt.Sprintf("æ¶ˆæ¯-%d", i)
		bus.Publisher().Publish(context.Background(), "demo.batch", []byte(msg))
	}

	for atomic.LoadInt64(&processed) < int64(messageCount) {
		time.Sleep(100 * time.Millisecond)
	}

	duration := time.Since(start)
	router.Stop()

	log.Printf("  æ‰¹é‡å¤„ç†: %d æ¡æ¶ˆæ¯ï¼Œè€—æ—¶ %v\n", messageCount, duration)
}

// ========== æ¼”ç¤º 4: å†…å­˜ä¼˜åŒ– ==========

func demonstrateMemoryOptimization() {
	log.Println("\nã€æ¼”ç¤º 4ã€‘å†…å­˜ä¼˜åŒ– - å¯¹è±¡æ± å’Œé›¶æ‹·è´")

	// ä½¿ç”¨ sync.Pool ä¼˜åŒ–å†…å­˜åˆ†é…
	msgPool := &sync.Pool{
		New: func() interface{} {
			return &messaging.Message{}
		},
	}

	log.Println("åœºæ™¯ 1: æ™®é€šåˆ›å»ºï¼ˆæ¯æ¬¡ newï¼‰")
	start1 := time.Now()
	for i := 0; i < 10000; i++ {
		msg := messaging.NewMessage("", []byte(fmt.Sprintf("æ¶ˆæ¯-%d", i)))
		_ = msg
	}
	duration1 := time.Since(start1)
	log.Printf("  æ™®é€šåˆ›å»º: 10000 æ¬¡ï¼Œè€—æ—¶ %v\n", duration1)

	log.Println("\nåœºæ™¯ 2: å¯¹è±¡æ± ï¼ˆå¤ç”¨å¯¹è±¡ï¼‰")
	start2 := time.Now()
	for i := 0; i < 10000; i++ {
		msg := msgPool.Get().(*messaging.Message)
		msg.Payload = []byte(fmt.Sprintf("æ¶ˆæ¯-%d", i))
		msgPool.Put(msg)
	}
	duration2 := time.Since(start2)
	log.Printf("  å¯¹è±¡æ± : 10000 æ¬¡ï¼Œè€—æ—¶ %v\n", duration2)

	improvement := float64(duration1-duration2) / float64(duration1) * 100
	log.Printf("  âœ… æ€§èƒ½æå‡: %.2f%%\n", improvement)
}

// æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼š
//
// 1. æ€§èƒ½ä¼˜åŒ–æ–¹å‘
//    â€¢ ååé‡ï¼ˆThroughputï¼‰: æ¯ç§’å¤„ç†çš„æ¶ˆæ¯æ•°
//    â€¢ å»¶è¿Ÿï¼ˆLatencyï¼‰: å•æ¡æ¶ˆæ¯çš„å¤„ç†æ—¶é—´
//    â€¢ å¹¶å‘åº¦ï¼ˆConcurrencyï¼‰: åŒæ—¶å¤„ç†çš„æ¶ˆæ¯æ•°
//    â€¢ å†…å­˜å ç”¨ï¼ˆMemoryï¼‰: å‡å°‘å†…å­˜åˆ†é…å’Œ GC
//
// 2. å¹¶å‘ä¼˜åŒ–
//    â€¢ å¤š Worker: æå‡å¹¶å‘å¤„ç†èƒ½åŠ›
//    â€¢ åç¨‹æ± : æ§åˆ¶å¹¶å‘æ•°ï¼Œé¿å…èµ„æºè€—å°½
//    â€¢ æ— é”æ•°æ®ç»“æ„: å‡å°‘é”ç«äº‰
//
// 3. æ‰¹é‡å¤„ç†
//    â€¢ æ‰¹é‡å†™å…¥æ•°æ®åº“: å‡å°‘ç½‘ç»œå¾€è¿”
//    â€¢ æ‰¹é‡è°ƒç”¨ API: å‡å°‘è¿æ¥å¼€é”€
//    â€¢ æ‰¹é‡å‘é€æ¶ˆæ¯: æå‡ååé‡
//
// 4. å†…å­˜ä¼˜åŒ–
//    â€¢ å¯¹è±¡æ± ï¼ˆsync.Poolï¼‰: å¤ç”¨å¯¹è±¡ï¼Œå‡å°‘ GC
//    â€¢ é›¶æ‹·è´: é¿å…ä¸å¿…è¦çš„å†…å­˜æ‹·è´
//    â€¢ é¢„åˆ†é…: æå‰åˆ†é…å†…å­˜ï¼Œé¿å…åŠ¨æ€æ‰©å®¹
//
// 5. ç½‘ç»œä¼˜åŒ–
//    â€¢ è¿æ¥æ± : å¤ç”¨è¿æ¥
//    â€¢ æ‰¹é‡å‘é€: å‡å°‘ç½‘ç»œ I/O
//    â€¢ å‹ç¼©: å‡å°‘ä¼ è¾“æ•°æ®é‡
//
// æ€§èƒ½åŸºå‡†ï¼š
//
// NSQ å•æœºæ€§èƒ½ï¼ˆå‚è€ƒï¼‰:
// â€¢ ååé‡: 10w+ msg/s
// â€¢ å»¶è¿Ÿ: P99 < 10ms
// â€¢ å†…å­˜: æ¯ 100w æ¶ˆæ¯ ~100MB
//
// ä¼˜åŒ–å‰ vs ä¼˜åŒ–å:
// â€¢ å• Worker: 1000 msg/s
// â€¢ 8 Worker: 8000 msg/sï¼ˆ8å€æå‡ï¼‰
// â€¢ æ‰¹é‡å¤„ç†: 5000 msg/s â†’ 15000 msg/sï¼ˆ3å€æå‡ï¼‰
// â€¢ å¯¹è±¡æ± : å‡å°‘ 50% å†…å­˜åˆ†é…
//
// æ€§èƒ½ç›‘æ§æŒ‡æ ‡ï¼š
//
// 1. ä¸šåŠ¡æŒ‡æ ‡
//    â€¢ æ¶ˆæ¯å¤„ç†é€Ÿç‡
//    â€¢ æ¶ˆæ¯ç§¯å‹æ•°é‡
//    â€¢ é”™è¯¯ç‡
//
// 2. ç³»ç»ŸæŒ‡æ ‡
//    â€¢ CPU ä½¿ç”¨ç‡
//    â€¢ å†…å­˜ä½¿ç”¨ç‡
//    â€¢ ç½‘ç»œå¸¦å®½
//    â€¢ ç£ç›˜ I/O
//
// 3. Go è¿è¡Œæ—¶æŒ‡æ ‡
//    â€¢ Goroutine æ•°é‡
//    â€¢ GC é¢‘ç‡å’Œè€—æ—¶
//    â€¢ å †å†…å­˜å¤§å°
//
// æ€§èƒ½ä¼˜åŒ–æµç¨‹ï¼š
//
// 1. åŸºå‡†æµ‹è¯•ï¼ˆBaselineï¼‰
//    â€¢ æµ‹é‡å½“å‰æ€§èƒ½
//    â€¢ ç¡®å®šç“¶é¢ˆ
//
// 2. æ€§èƒ½åˆ†æï¼ˆProfilingï¼‰
//    â€¢ CPU Profile
//    â€¢ Memory Profile
//    â€¢ Goroutine Profile
//    â€¢ Block Profile
//
// 3. é’ˆå¯¹æ€§ä¼˜åŒ–
//    â€¢ ä¼˜åŒ–çƒ­ç‚¹ä»£ç 
//    â€¢ å‡å°‘å†…å­˜åˆ†é…
//    â€¢ ä¼˜åŒ–ç®—æ³•
//
// 4. éªŒè¯æ•ˆæœ
//    â€¢ å†æ¬¡åŸºå‡†æµ‹è¯•
//    â€¢ å¯¹æ¯”ä¼˜åŒ–å‰å
//
// Go æ€§èƒ½åˆ†æå·¥å…·ï¼š
//
// 1. pprof
//    ```go
//    import _ "net/http/pprof"
//    go func() {
//        http.ListenAndServe("localhost:6060", nil)
//    }()
//    ```
//
//    è®¿é—®: http://localhost:6060/debug/pprof/
//
// 2. trace
//    ```go
//    import "runtime/trace"
//
//    f, _ := os.Create("trace.out")
//    trace.Start(f)
//    defer trace.Stop()
//    ```
//
// 3. benchstat
//    ```bash
//    go test -bench=. -count=10 > old.txt
//    # ä¼˜åŒ–ä»£ç 
//    go test -bench=. -count=10 > new.txt
//    benchstat old.txt new.txt
//    ```
//
// å‹æµ‹å·¥å…·ï¼š
//
// 1. å†…ç½®å‹æµ‹
//    ```bash
//    go test -bench=. -benchmem
//    ```
//
// 2. wrkï¼ˆHTTPï¼‰
//    ```bash
//    wrk -t4 -c100 -d30s http://localhost:8080/
//    ```
//
// 3. è‡ªå®šä¹‰å‹æµ‹è„šæœ¬
//    â€¢ æ¨¡æ‹ŸçœŸå®è´Ÿè½½
//    â€¢ å¤šåœºæ™¯æµ‹è¯•
//
// æœ€ä½³å®è·µï¼š
// âœ… å…ˆæµ‹é‡å†ä¼˜åŒ–ï¼ˆä¸è¦è¿‡æ—©ä¼˜åŒ–ï¼‰
// âœ… ä¼˜åŒ–çƒ­ç‚¹ä»£ç ï¼ˆ80/20 åŸåˆ™ï¼‰
// âœ… ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·å®šä½ç“¶é¢ˆ
// âœ… æ‰¹é‡æ“ä½œä¼˜äºå•æ¡æ“ä½œ
// âœ… æ§åˆ¶å¹¶å‘åº¦ï¼ˆä¸æ˜¯è¶Šå¤šè¶Šå¥½ï¼‰
// âœ… ä½¿ç”¨å¯¹è±¡æ± å‡å°‘å†…å­˜åˆ†é…
// âœ… å‹æµ‹è¦æ¨¡æ‹ŸçœŸå®è´Ÿè½½
// âœ… æŒç»­ç›‘æ§ç”Ÿäº§ç¯å¢ƒæ€§èƒ½
//
// æ³¨æ„äº‹é¡¹ï¼š
// âš ï¸ ä¼˜åŒ–è¦æœ‰æ˜ç¡®ç›®æ ‡ï¼ˆååé‡è¿˜æ˜¯å»¶è¿Ÿï¼‰
// âš ï¸ è¿‡åº¦ä¼˜åŒ–ä¼šå¢åŠ ä»£ç å¤æ‚åº¦
// âš ï¸ å¹¶å‘ä¸æ˜¯è¶Šå¤šè¶Šå¥½ï¼ˆè¦å¹³è¡¡èµ„æºï¼‰
// âš ï¸ æ‰¹é‡å¤„ç†ä¼šå¢åŠ å»¶è¿Ÿ
// âš ï¸ å¯¹è±¡æ± è¦æ­£ç¡®ä½¿ç”¨ï¼ˆé¿å…çŠ¶æ€æ±¡æŸ“ï¼‰
// âš ï¸ å‹æµ‹ç¯å¢ƒè¦æ¥è¿‘ç”Ÿäº§ç¯å¢ƒ
// âš ï¸ æ€§èƒ½ä¼˜åŒ–è¦è€ƒè™‘å¯ç»´æŠ¤æ€§
//
// æ€§èƒ½è°ƒä¼˜æ¸…å•ï¼š
// â–¡ ä½¿ç”¨å¤š Worker å¹¶è¡Œå¤„ç†
// â–¡ æ‰¹é‡æ“ä½œï¼ˆæ•°æ®åº“ã€APIï¼‰
// â–¡ ä½¿ç”¨å¯¹è±¡æ± ï¼ˆsync.Poolï¼‰
// â–¡ é¢„åˆ†é…åˆ‡ç‰‡å®¹é‡
// â–¡ é¿å…ä¸å¿…è¦çš„å†…å­˜æ‹·è´
// â–¡ ä½¿ç”¨è¿æ¥æ± 
// â–¡ å¯ç”¨æ¶ˆæ¯å‹ç¼©ï¼ˆè·¨æ•°æ®ä¸­å¿ƒï¼‰
// â–¡ è°ƒæ•´ GC å‚æ•°ï¼ˆGOGCï¼‰
// â–¡ ä½¿ç”¨ç¼“å­˜ï¼ˆæœ¬åœ°ç¼“å­˜ã€Redisï¼‰
// â–¡ å¼‚æ­¥å¤„ç†éå…³é”®è·¯å¾„
